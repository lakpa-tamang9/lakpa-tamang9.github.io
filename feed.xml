<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://lakpa-tamang9.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://lakpa-tamang9.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-09-12T01:01:08+00:00</updated><id>https://lakpa-tamang9.github.io/feed.xml</id><title type="html">blank</title><entry><title type="html">Variational Bayesian Inference</title><link href="https://lakpa-tamang9.github.io/blog/2024/variational-bayesian-inference/" rel="alternate" type="text/html" title="Variational Bayesian Inference"/><published>2024-05-18T06:41:00+00:00</published><updated>2024-05-18T06:41:00+00:00</updated><id>https://lakpa-tamang9.github.io/blog/2024/variational-bayesian-inference</id><content type="html" xml:base="https://lakpa-tamang9.github.io/blog/2024/variational-bayesian-inference/"><![CDATA[<p><strong>Log likelihood</strong> is a statistical measure that is used to quantify how well a given set of parameters explains the observed data under specific probabilistic model. It is especially popular in statistics and machine learning community, particularly in the context of Bayesian inference, which this post is about. In simpler words, Likelihood : How probable is the observed data given a set of parameters?, and Log-likelihoo: The natural logarithm of the likelihood, transforming a product of proabilities into a sum of log-probabilities for ease of computation.</p> <p>Now let’s break down the log likelihood to more thorough understanding with an example: Let us consider, we have some observed data points \(x_1, x_2, \cdots, x_n\). Also, let’s assume that these data points are generated from a probability distribution with parameters \(\theta\). For example, the data might come from a normal distribution with mean \(\mu\), and standard deviation \(\sigma\) such that \(\theta = (\mu, \sigma)\).</p> <p>Now the likelihood function \(L(\theta)\) is the probability of the observed data given the parameters \(\theta\), or mathematically:</p> \[L(\theta) = P(X \mid \theta)\] <p>For \(n\) independent and identically distributed (i.i.d) observations \(x_1, x_2, \cdots, x_n\), the likelihood function is defined as the product of the probabilities of each individual observation. Mathematically,</p> \[L(\theta) = \prod_{i = 1}^{n} P(x_i \mid \theta)\] <p>Subsequently, the <strong>Log-Likelihood Function</strong> is now simply the natural logarithm of the above likelihood function.</p> \[\mathcal{l}(\theta) = \sum_{i = 1}^n \log L(\theta) = log(\prod_{i = 1}^n)P(x_i \mid \theta)\] <p>Using the properties of logarithm, the above equation can be expressed as sum. This transformation from product to sum is often done because mathematically sums are easier to work with compared to products, especially when taking gradients (eg: for optimization).</p> \[\mathcal{l}(\theta) = \sum_{i = 1}^n \log P(x_i \mid \theta)\] <h1 id="log-likelihood-for-a-normal-distribution">Log-Likelihood for a Normal Distribution</h1> <p>Usually in scenarios when the data is modelled as a Normal (Gaussian) distribution with some mean \(\mu\), and standard deviation \(\sigma\), the conditional probability of some input \(x_i\) is given by:</p> \[P(x_i \mid \mu, \sigma) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp(\frac{-(x_i - \mu)^2}{2\sigma^2})\] <p>The log-likelihood function for this normal distribution can be written as:</p> \[\mathcal{l}(\mu, \sigma) = \sum_{i = 1}^n \log(\frac{1}{\sqrt{2\pi\sigma^2}}\exp(\frac{-(x_i - \mu)^2}{2\sigma^2}))\] \[\mathcal{l}(\mu, \sigma) = \sum_{i = 1}^n(log(\frac{1}{\sqrt{2\pi\sigma^2}}) + \log(\exp(\frac{-(x_i - \mu)^2}{2\sigma^2})))\] \[\mathcal{l}(\mu, \sigma) = \sum_{i = 1}^n(-\frac{1}{2}\log(2\pi\sigma^2) - \frac{-(x_i - \mu)^2}{2\sigma^2})\] <h1 id="bayesian-linear-layer">Bayesian Linear Layer</h1> <p>Now, let us create a Bayesian linear layer in Pytorch. The weights of the linear layer are sampled from a fully factorised Normal with learnable parameters. The likelihood of the weight samples under the prior and the approximate posterior are returned with each forward pass in order to estimate the KL term in the ELBO.</p> <p>In the following code, a Python Class created that inherits from nn.Module. For easier understanding of the process, I will try to break down and explain the code snippets in simple manner.</p> <ul> <li>First, the layer is initialized with the number of input features <code class="language-plaintext highlighter-rouge">num_inp_feats</code>, the number of output features <code class="language-plaintext highlighter-rouge">num_out_feats</code>, a prior distribution class <code class="language-plaintext highlighter-rouge">prior_class</code>, and an optional bias term <code class="language-plaintext highlighter-rouge">with_bias</code> (default is <code class="language-plaintext highlighter-rouge">True</code>).</li> <li><code class="language-plaintext highlighter-rouge">self.W_mu</code> and <code class="language-plaintext highlighter-rouge">self.W_p</code> are the mean and a parameter related to the standard deviation of the weights, respectively.</li> <li><code class="language-plaintext highlighter-rouge">W_mu</code> is initialized uniformly between -0.1 and 0.1.</li> <li><code class="language-plaintext highlighter-rouge">W_p</code> is initialized uniformly between -3 and -2.</li> <li><code class="language-plaintext highlighter-rouge">self.b_mu</code> and <code class="language-plaintext highlighter-rouge">self.b_p</code> are the mean and a parameter related to the standard deviation of the biases, respectively, initialized similarly to the weights.</li> </ul> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>class BayesLinearNormalDist(nn.Module):

    def __init__(self, num_inp_feats, num_out_feats, prior_class, with_bias=True):
        super(BayesLinearNormalDist, self).__init__()
        self.num_inp_feats = num_inp_feats
        self.num_out_feats = num_out_feats
        self.prior = prior_class
        self.with_bias = with_bias

        # Learnable parameters -&gt; Initialisation is set empirically.
        self.W_mu = nn.Parameter(
            torch.Tensor(self.num_inp_feats, self.num_out_feats).uniform_(-0.1, 0.1)
        )
        self.W_p = nn.Parameter(
            torch.Tensor(self.num_inp_feats, self.num_out_feats).uniform_(-3, -2)
        )

        self.b_mu = nn.Parameter(torch.Tensor(self.num_out_feats).uniform_(-0.1, 0.1))
        self.b_p = nn.Parameter(torch.Tensor(self.num_out_feats).uniform_(-3, -2))

</code></pre></div></div> <p>The forward method below defines the forward pass of the layer.</p> <ul> <li>If <code class="language-plaintext highlighter-rouge">ifsample</code> is <code class="language-plaintext highlighter-rouge">False</code>, the method returns the deterministic output using the mean of the weights and biases.</li> <li>If <code class="language-plaintext highlighter-rouge">ifsample</code> is <code class="language-plaintext highlighter-rouge">True</code>, it proceeds to sample weights and biases from the approximate posterior distribution rather than using the mean values. This stochastic sampling is crucial for Bayesian neural networks, allowing the model to account for uncertainty in the parameters. <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  def forward(self, X, sample=0, local_rep=False, ifsample=True):

      if not ifsample:  # When training return MLE of w for quick validation
          # pdb.set_trace()
          if self.with_bias:
              output = torch.mm(X, self.W_mu) + self.b_mu.expand(
                  X.size()[0], self.num_out_feats
              )
          else:
              output = torch.mm(X, self.W_mu)
          return output, torch.Tensor([0]).cuda()
</code></pre></div> </div> <p>The weights and biases are expanded along a new dimension for sampling.</p> </li> <li><code class="language-plaintext highlighter-rouge">eps_W</code> and <code class="language-plaintext highlighter-rouge">eps_b</code> are standard normal noise samples.</li> <li><code class="language-plaintext highlighter-rouge">std_w</code> and <code class="language-plaintext highlighter-rouge">std_b</code> are computed using the softplus function to ensure positivity.</li> <li><code class="language-plaintext highlighter-rouge">W</code> and <code class="language-plaintext highlighter-rouge">b</code> are sampled from the approximate posterior.</li> <li><em>Unsqueezing</em>: The unsqueeze method adds an extra dimension to the tensors. This is necessary to prepare the tensors for sampling. <ul> <li><code class="language-plaintext highlighter-rouge">W_mu</code> and <code class="language-plaintext highlighter-rouge">W_p</code> are unsqueezed at dimension 1, transforming them from shape (<code class="language-plaintext highlighter-rouge">num_inp_feats</code>, <code class="language-plaintext highlighter-rouge">num_out_feats</code>) to (<code class="language-plaintext highlighter-rouge">num_inp_feats</code>, 1, <code class="language-plaintext highlighter-rouge">num_out_feats</code>).</li> <li><code class="language-plaintext highlighter-rouge">b_mu</code> and <code class="language-plaintext highlighter-rouge">b_p</code> are unsqueezed at dimension 0, transforming them from shape (<code class="language-plaintext highlighter-rouge">num_out_feats</code>) to (1, <code class="language-plaintext highlighter-rouge">num_out_feats</code>).</li> </ul> </li> <li> <p><em>Repeating</em>: The repeat method replicates the tensors along specified dimensions.This ensures that each weight and bias parameter has multiple samples, one for each desired sample.</p> <ul> <li><code class="language-plaintext highlighter-rouge">W_mu</code> and <code class="language-plaintext highlighter-rouge">W_p</code> are repeated along the new dimension to match the number of samples, resulting in shape (<code class="language-plaintext highlighter-rouge">num_inp_feats</code>, <code class="language-plaintext highlighter-rouge">sample</code>, <code class="language-plaintext highlighter-rouge">num_out_feats</code>).</li> <li><code class="language-plaintext highlighter-rouge">b_mu</code> and <code class="language-plaintext highlighter-rouge">b_p</code> are repeated along the new dimension to match the number of samples, resulting in shape (<code class="language-plaintext highlighter-rouge">sample</code>, <code class="language-plaintext highlighter-rouge">num_out_feats</code>).</li> </ul> </li> <li><em>Sampling Noise</em>: <code class="language-plaintext highlighter-rouge">eps_W</code> and <code class="language-plaintext highlighter-rouge">eps_b</code> are tensors of the same shape as <code class="language-plaintext highlighter-rouge">W_mu</code> and <code class="language-plaintext highlighter-rouge">b_mu</code>, filled with samples from a standard normal distribution.</li> <li><em>Computing Standard Deviation</em>: The standard deviations for weights and biases are computed using the <em>softplus</em> function applied to <code class="language-plaintext highlighter-rouge">W_p</code> and <code class="language-plaintext highlighter-rouge">b_p</code>. The softplus function ensures that the standard deviations are positive.</li> <li>The weights <code class="language-plaintext highlighter-rouge">W</code> and biases <code class="language-plaintext highlighter-rouge">b</code> are sampled from their approximate posterior distributions by adding the scaled noise (<code class="language-plaintext highlighter-rouge">std_w</code> * <code class="language-plaintext highlighter-rouge">eps_W</code> and <code class="language-plaintext highlighter-rouge">std_b</code> * <code class="language-plaintext highlighter-rouge">eps_b</code>) to their means (<code class="language-plaintext highlighter-rouge">W_mu</code> and <code class="language-plaintext highlighter-rouge">b_mu</code>). <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>      else:
          if not local_rep:
              # Tensor.new()  Constructs a new tensor of the same data type as self tensor.
              # the same random sample is used for every element in the minibatch
              # pdb.set_trace()
              W_mu = self.W_mu.unsqueeze(1).repeat(1, sample, 1)
              W_p = self.W_p.unsqueeze(1).repeat(1, sample, 1)

              b_mu = self.b_mu.unsqueeze(0).repeat(sample, 1)
              b_p = self.b_p.unsqueeze(0).repeat(sample, 1)

              eps_W = W_mu.data.new(W_mu.size()).normal_()
              eps_b = b_mu.data.new(b_mu.size()).normal_()

              if not ifsample:
                  eps_W = eps_W * 0
                  eps_b = eps_b * 0

              # sample parameters
              std_w = 1e-6 + f.softplus(W_p, beta=1, threshold=20)
              std_b = 1e-6 + f.softplus(b_p, beta=1, threshold=20)

              W = W_mu + 1 * std_w * eps_W
              b = b_mu + 1 * std_b * eps_b
</code></pre></div> </div> </li> <li><code class="language-plaintext highlighter-rouge">lqw</code> and <code class="language-plaintext highlighter-rouge">lpw</code> are the log-likelihoods of the weights and biases under the approximate posterior and prior distributions, respectively.</li> <li> <p><strong>Log-Likelihood under the Posterior (lqw)</strong>: This is computed for both weights and biases if <code class="language-plaintext highlighter-rouge">self.with_bias</code> is <code class="language-plaintext highlighter-rouge">True</code>. The function <code class="language-plaintext highlighter-rouge">isotropic_gauss_loglike</code> calculates the log-likelihood of the sampled parameters under the approximate posterior distribution.</p> </li> <li><strong>Log-Likelihood under the Prior (lpw)</strong>: This is computed using the provided prior class for both weights and biases if <code class="language-plaintext highlighter-rouge">self.with_bias</code> is <code class="language-plaintext highlighter-rouge">True</code>.</li> </ul> <p>For log-likelihood under the posterior, the weight and bias parameter including the mean, and standard deviation of weights are passed as arguments to <code class="language-plaintext highlighter-rouge">isotropic_gauss_loglike</code> function. The <code class="language-plaintext highlighter-rouge">isotropic_gauss_loglike</code> function computes the log-likelihood of a set of samples <code class="language-plaintext highlighter-rouge">x</code> under an isotropic Gaussian (Normal) distribution with mean <code class="language-plaintext highlighter-rouge">mu</code> and standard deviation <code class="language-plaintext highlighter-rouge">sigma</code>. Within the <code class="language-plaintext highlighter-rouge">BayesLinearNormalDist</code> class, the <code class="language-plaintext highlighter-rouge">isotropic_gauss_loglike</code> function is used to calculate the log-likelihood of the sampled weights and biases under the approximate posterior distribution. This helps in computing the <em>Evidence Lower Bound (ELBO)</em> for variational inference. Specifically, the log-likelihood difference between the posterior and prior distributions (<code class="language-plaintext highlighter-rouge">lqw</code> - <code class="language-plaintext highlighter-rouge">lpw</code>) is used as part of the loss function to optimize the parameters of the model.Here is a detailed breakdown of the method:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def isotropic_gauss_loglike(x, mu, sigma, do_sum=True):
    """Returns the computed log-likelihood

    Args:
        x (_type_): the sampled weights or biases
        mu (_type_): mean of gaussian distribution
        sigma (_type_): standard deviation of gaussian dist
        do_sum (bool, optional): _description_. a boolean indicating whether to sum the log-likelihoods
        over all elements or to take the mean.

    Returns:
        _type_: Gaussian Log likelihood
    """
    cte_term = -(0.5) * np.log(2 * np.pi)   # constant term
    det_sig_term = -torch.log(sigma)    # Determinant term
    inner = (x - mu) / sigma
    dist_term = -(0.5) * (inner**2)

    if do_sum:
        out = (cte_term + det_sig_term + dist_term).sum()  # sum over all weights
    else:
        out = (cte_term + det_sig_term + dist_term).mean()
    return out
</code></pre></div></div> <ul> <li><code class="language-plaintext highlighter-rouge">cte_term</code> : Constant term, This term is a constant that is part of the Gaussian log-likelihood formula. It arises from the normalization constant of the Gaussian distribution.</li> <li><code class="language-plaintext highlighter-rouge">det_sig_term</code>: Determinant term, This term accounts for the log of the determinant of the covariance matrix. In the isotropic case (where the covariance matrix is diagonal with equal entries), this term simplifies to the log of the standard deviation.</li> <li>dist_term: Distance term, This term measures the squared distance between the samples x and the mean mu, scaled by the standard deviation sigma. The result is then multiplied by -0.5, which is part of the Gaussian log-likelihood formula.</li> <li>The output is computed using the sampled weights and biases.</li> </ul> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>                if self.with_bias:
                    lqw = isotropic_gauss_loglike(
                        W, W_mu, std_w
                    ) + isotropic_gauss_loglike(b, b_mu, std_b)
                    lpw = self.prior.loglike(W) + self.prior.loglike(b)
                else:
                    lqw = isotropic_gauss_loglike(W, W_mu, std_w)
                    lpw = self.prior.loglike(W)

                # Reshaping weight to (num_inp_feats, num_out_feats) and biases to (num_out_feats)
                W = W.view(W.size()[0], -1)
                b = b.view(-1)

                if self.with_bias:
                    # wx + b
                    output = torch.mm(X, W) + b.unsqueeze(0).expand(
                        X.shape[0], -1
                    )  # (batch_size, num_out_featsput)
                else:
                    output = torch.mm(X, W)
</code></pre></div></div> <p>If local representation <code class="language-plaintext highlighter-rouge">local_rep</code> is <code class="language-plaintext highlighter-rouge">True</code>, weights and biases are sampled for each data point in the batch. The process becomes slightly different, with weights and biases sampled for each data point individually rather than a shared sample across the batch. The output is computed using batch matrix multiplication (<code class="language-plaintext highlighter-rouge">torch.bmm</code>). The method returns the output and the difference between the log-likelihoods of the weights and biases under the posterior and prior distributions.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>            else:
                W_mu = self.W_mu.unsqueeze(0).repeat(X.size()[0], 1, 1)
                W_p = self.W_p.unsqueeze(0).repeat(X.size()[0], 1, 1)

                b_mu = self.b_mu.unsqueeze(0).repeat(X.size()[0], 1)
                b_p = self.b_p.unsqueeze(0).repeat(X.size()[0], 1)
                # pdb.set_trace()
                eps_W = W_mu.data.new(W_mu.size()).normal_()
                eps_b = b_mu.data.new(b_mu.size()).normal_()

                # sample parameters
                std_w = 1e-6 + f.softplus(W_p, beta=1, threshold=20)
                std_b = 1e-6 + f.softplus(b_p, beta=1, threshold=20)

                W = W_mu + 1 * std_w * eps_W
                b = b_mu + 1 * std_b * eps_b

                # W = W.view(W.size()[0], -1)
                # b = b.view(-1)
                # pdb.set_trace()

                if self.with_bias:
                    output = (
                        torch.bmm(X.view(X.size()[0], 1, X.size()[1]), W).squeeze() + b
                    )  # (batch_size, num_out_featsput)
                    lqw = isotropic_gauss_loglike(
                        W, W_mu, std_w
                    ) + isotropic_gauss_loglike(b, b_mu, std_b)
                    lpw = self.prior.loglike(W) + self.prior.loglike(b)
                else:
                    output = torch.bmm(X.view(X.size()[0], 1, X.size()[1]), W).squeeze()
                    lqw = isotropic_gauss_loglike(W, W_mu, std_w)
                    lpw = self.prior.loglike(W)

            return output, lqw - lpw
</code></pre></div></div> <h1 id="variational-inference-and-the-elbo">Variational Inference and the ELBO</h1> <p>In Bayesian inference, the goal is to estimate the posterior distribution of the model parameters given the data. However, directly computing the posterior is often intractable for complex models, so we use variational inference as an approximation technique.</p> <p><strong>Evidence Lower Bound (ELBO)</strong> The Evidence Lower Bound (ELBO) is a key concept in variational inference. It provides a way to approximate the true posterior distribution by optimizing a simpler, parameterized distribution. The ELBO is defined as:</p> \[ELBO = \EX_{q(\theta)}[\log P(X \mid \theta)] - KL(q(\theta) \| P(\theta))\] <ul> <li>where, \(\EX_{q(\theta)}[\log P(X \mid \theta)]\) is the expected log-likelihood of the data under the approximate posterior \(q(\theta)\).</li> <li>\(KL(q(\theta) \| P(\theta))\) is the Kullback-Leibler divergence between the approximate posterior \(q(\theta)\) and the prior \(P(\theta)\).</li> </ul> <p>The ELBO serves two main purposes:</p> <ol> <li>Maximizing the Data Likelihood: By maximizing the first term, we ensure that the model parameters explain the data well.</li> <li>Regularizing with the Prior: By minimizing the KL divergence, we ensure that the approximate posterior stays close to the prior, preventing overfitting.</li> </ol> <p>In the <code class="language-plaintext highlighter-rouge">BayesLinearNormalDist</code> class discussed above, the forward method returns (<code class="language-plaintext highlighter-rouge">lqw</code> - <code class="language-plaintext highlighter-rouge">lpw</code>). This term represents the contribution of the KL divergence for the sampled weights and biases. By returning (<code class="language-plaintext highlighter-rouge">lqw</code> − <code class="language-plaintext highlighter-rouge">lpw</code>), the method provides the necessary components to compute the KL divergence term in the ELBO. This term is crucial for ensuring that the approximate posterior does not deviate too much from the prior.</p> <p><strong>In practice</strong>, The loss function to be minimized during training is the negative ELBO.</p> \[Loss = -ELBO = -(\EX_{q(\theta)}[\log P(X \mid \theta)] - KL(q(\theta) \| P(\theta)))\] <h1 id="mixture-of-gaussian-prior">Mixture of Gaussian Prior</h1> <p>The <code class="language-plaintext highlighter-rouge">isotropic_mixture_gauss_prior</code> class below defines a prior distribution that is a mixture of two isotropic Gaussian distributions. This means that the prior distribution for the weights and biases is not a single Gaussian, but a weighted combination of two Gaussians with different means and standard deviations. This type of prior can capture more complex prior beliefs about the parameters.</p> <p>In the <code class="language-plaintext highlighter-rouge">prior_class</code> argument in <code class="language-plaintext highlighter-rouge">BayesLinearNormalDist</code> class, the mixture of gaussian priors is considered. Let’s break down this code:</p> <ul> <li><strong>Parameters</strong>: <ul> <li><code class="language-plaintext highlighter-rouge">mu1</code>, <code class="language-plaintext highlighter-rouge">sigma1</code>: Mean and standard deviation of the first Gaussian component.</li> <li><code class="language-plaintext highlighter-rouge">mu2</code>, <code class="language-plaintext highlighter-rouge">sigma2</code>: Mean and standard deviation of the second Gaussian component.</li> <li><code class="language-plaintext highlighter-rouge">pi</code>: Mixing coefficient for the first Gaussian component (probability that a sample comes from the first Gaussian). The second component’s weight is 1 - <code class="language-plaintext highlighter-rouge">pi</code>.</li> </ul> </li> </ul> <p>– <strong>Precomputed Terms</strong>: - <code class="language-plaintext highlighter-rouge">cte_term</code>: Constant term from the Gaussian log-likelihood. - <code class="language-plaintext highlighter-rouge">det_sig_term1</code>, <code class="language-plaintext highlighter-rouge">det_sig_term2</code>: Logarithms of the standard deviations for the two Gaussian components.</p> <ul> <li><strong>Distance Terms</strong>: <ul> <li><code class="language-plaintext highlighter-rouge">dist_term1</code>: Distance term for the first Gaussian, measuring how far <code class="language-plaintext highlighter-rouge">x</code> is from <code class="language-plaintext highlighter-rouge">mu1</code> scaled by <code class="language-plaintext highlighter-rouge">sigma1</code>.</li> <li><code class="language-plaintext highlighter-rouge">dist_term2</code>: Distance term for the second Gaussian, measuring how far <code class="language-plaintext highlighter-rouge">x</code> is from <code class="language-plaintext highlighter-rouge">mu2</code> scaled by <code class="language-plaintext highlighter-rouge">sigma2</code>.</li> </ul> </li> <li><strong>Mixture Log-Likelihood</strong>: <ul> <li>The log-likelihood is computed as the log of a weighted sum of the exponentiated terms from both Gaussian components.</li> <li>If <code class="language-plaintext highlighter-rouge">do_sum</code> is <code class="language-plaintext highlighter-rouge">True</code>, the log-likelihoods are summed over all elements. Otherwise, the mean is taken.</li> </ul> </li> </ul> <p><strong>Mathematical Explanation</strong> For each sampled parameter \(x\) (weights, or biases), the log likelihood under the mixture prior is computed as:</p> \[\log P(x) = \log(\pi_1\dot\exp(\frac{-(x-\mu_1)^2}{2\sigma_1^2}) + (1 - \pi_1)\dot\exp(\frac{-(x-\mu_2)^2}{2\sigma_2^2}))\] <p>where,</p> <ul> <li>\(\pi_1\), and \(1 - \pi_1\) are the mixing coefficients.</li> <li>\(\mu_1, \sigma_1, \mu_2, \sigma_2\) are the means and standard deviations of two Gaussian components.</li> </ul> <p><strong>Significance in variational Inference</strong>: The log-likelihood under the prior <code class="language-plaintext highlighter-rouge">lpw</code> is crucial for computing the KL divergence term in the ELBO.</p> \[KL(q(\theta) \| P(\theta)) = \EX_{q(\theta)}[\log q(\theta) - \log P(\theta)]\] <p>The <code class="language-plaintext highlighter-rouge">isotropic_mixture_gauss_prior</code> class defines a prior as a mixture of two Gaussians, and its loglike method computes the log-likelihood of parameters under this mixture prior. In the <code class="language-plaintext highlighter-rouge">BayesLinearNormalDist</code> class, this log-likelihood is used to compute the KL divergence term in the ELBO, balancing the fit of the model to the data with the regularization imposed by the prior.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>class isotropic_mixture_gauss_prior(object):
    def __init__(self, mu1=0, mu2=0, sigma1=0.1, sigma2=1.5, pi=0.5):
        self.mu1 = mu1
        self.sigma1 = sigma1
        self.mu2 = mu2
        self.sigma2 = sigma2
        self.pi1 = pi
        self.pi2 = 1 - pi

        self.cte_term = -(0.5) * np.log(2 * np.pi)

        self.det_sig_term1 = -np.log(self.sigma1)

        self.det_sig_term2 = -np.log(self.sigma2)

    def loglike(self, x, do_sum=True):

        dist_term1 = -(0.5) * ((x - self.mu1) / self.sigma1) ** 2
        dist_term2 = -(0.5) * ((x - self.mu2) / self.sigma2) ** 2

        if do_sum:
            return (
                torch.log(
                    self.pi1
                    * torch.exp(self.cte_term + self.det_sig_term1 + dist_term1)
                    + self.pi2
                    * torch.exp(self.cte_term + self.det_sig_term2 + dist_term2)
                )
            ).sum()
        else:
            return (
                torch.log(
                    self.pi1
                    * torch.exp(self.cte_term + self.det_sig_term1 + dist_term1)
                    + self.pi2
                    * torch.exp(self.cte_term + self.det_sig_term2 + dist_term2)
                )
            ).mean()
</code></pre></div></div>]]></content><author><name></name></author><category term="blog-posts"/><category term="bayesian"/><category term="inference,"/><category term="ELBO"/><summary type="html"><![CDATA[Using variational inference in bayesian networks..]]></summary></entry><entry><title type="html">Probability in Machine Learning</title><link href="https://lakpa-tamang9.github.io/blog/2024/prob-in-ml-2/" rel="alternate" type="text/html" title="Probability in Machine Learning"/><published>2024-04-14T08:12:00+00:00</published><updated>2024-04-14T08:12:00+00:00</updated><id>https://lakpa-tamang9.github.io/blog/2024/prob-in-ml-2</id><content type="html" xml:base="https://lakpa-tamang9.github.io/blog/2024/prob-in-ml-2/"><![CDATA[<p>Machine learning is highly data driven. By data driven, it means that the goal of machine learning is to design a general purpose methodology to extract valuable patterns from data, ideally without much domain-specific expertise. A machine learning model is said to learn from the data if its performance on a given task improves after the data is taken into account. The ultimate goal is to find good models through learning from data, that generalize well to yet unseen data, which we may care about in the future. Learning can be viewed as a way to automatically find patterns and structure in the data by optimizing the model parameters. Training the model means to use the available data to optimize some parameters of the model with resect to a utility function that evaluates how well the model predicts the training data. For instance, a training method can be though as an approach anlagous to climbing a hill to reach its peak. The peak here is the maximum of some desired performance.</p> <h1 id="what-is-probability">What is probability?</h1> <p>In a nutshell, “Probability is the study of uncertainity”. It is a degree of belief about an event occuring. We often quantify (measure) uncertainity in data, uncertainity in machine learning model, and uncertainity in predictions of the model. To quantify the uncertainity, the idea of <em>random variable</em> is required, which is a function that maps the outcomes of random experiments to a set of properties that we are interested in. For example, a coin is flipped two times and the record if it shows a head or a tail. The outcome or the sample space \(S = \left\{ HH, HT, TH, TT \right\}\). There are total of four possible outcomes and each of them is equaly likely. Instead of considering all the possible outcomes, we can consider assigning the variable $X$, to be the number of heads obtained in two flips of a coin. This translates to \(X\), a random variable taking on possible values of \(0, 1\) or \(2\). It is the probabilities of these values that we are interested in. i.e., the probabilities of head coming none, once or twice on two flips of a coin.</p> <h1 id="notations">Notations</h1> <ul> <li>For any two random variables \(X\) and \(Y\), the probability that \(X=x\), and \(Y=y\) is written as \(p(x, y)\) and is called the <em>joint probability</em>.</li> <li>The <em>marginal probability</em> that \(X\) takes the value \(x\) irrespective of the value of random variable $Y$ is written as \(p(x)\).</li> <li>\(X \sim p(x)\) denotes that the random variable \(X\) is distributed according to \(p(x)\).</li> <li>If we consider only the instances where \(X=x\), then fraction of instance (<em>conditional probability</em>) for which \(Y=y\) is written as \(p(y \mid x)\).</li> </ul> <h2 id="probability-density-function">Probability Density function</h2> <p>A function \(f: \mathbb{R} ^D \to \mathbb{R}\) is called a <em>probability density function (pdf)</em> if :</p> <ul> <li>\(\forall x \in \mathbb{R}^D : f(x) \ge 0\),</li> <li>Its integral exists, and</li> <li>\(\int_{\mathbb{R}^D}f(x)dx =1\).</li> </ul> <h3 id="simple-explanation-with-example">Simple Explanation with example</h3> <p>Imagine we have a smoothie shop that sells smoothies in cups of any size between 0 and 20 ounces. A PDF for the sizes of smoothies sold would show on a graph how likely it is to sell a smoothie at each size. For example, if most people buy 12-ounce smoothies, the graph would be higher at 12 ounces, indicating this size is more popular or likely to be sold.</p> <p><img src="/Users/lakpa/devs/personal_devs/lakpa-tamang9.github.io/assets/img/pdf.png" alt="Probability Density Function graph"/></p> <h3 id="key-points">Key Points</h3> <ul> <li><strong>Shape of the Graph</strong>: The shape tells you which outcomes are more likely. A peak at a particular value means that outcome is very likely.</li> <li><strong>Area Under the Graph</strong>: For any segment of the graph, the area under it (from the graph to the horizontal axis) represents the probability of outcomes within that range. For example, the area under the curve between 10 and 15 ounces tells you the likelihood of selling a smoothie that’s between 10 and 15 ounces.</li> <li><strong>Total Area Equals 1</strong>: The total area under the entire curve equals 1, which in probability terms means “100% certainty.” This is because some outcome within the possible range must happen.</li> </ul> <p>So, a PDF is essentially a way to visualize probabilities for continuous data, helping us see what outcomes are likely, how likely they are, and the range of possible outcomes.</p> <h2 id="sum-rule-product-rule-and-bayes-theorem">Sum rule, Product rule, and Bayes’ Theorem</h2> <p>Given the definitions of marginal, and conditional probability distributions, we can present the two fundamental rule in probability theory. The <em>sum rule</em> states that:</p> \[p(x)= \begin{cases} \sum_{y\in \mathcal{Y}} p(x, y),&amp; \text{if } y \text{ is discrete}\\ \int_\mathcal{Y}p(x, y)dy, &amp; \text{if } y \text{ is continuous} \end{cases} \label{eq:1}\] <p>where \(\mathcal{Y}\) are the states of the target space of random variable \(Y\). We sum out (or integrate out) the set of states $y$ of the random variable \(Y\). The sum rule is also known as the <em>marginalization property</em>. The sum rule relates the joint distribution to a marginal distribution.</p> <p>The <em>product rule</em> relates the joint distribution to the conditional distribution via\</p> \[p(x, y)=p(y \mid x)p(x)\label{eq:2}\] <p>The <em>product rule</em> can be interpreted as the fact that every joint distribution of two random variables can be factorized (written as a product) of two other distributions. These two factors are the marginal distributions of the first random variable $ p(x) $ and the conditional distribution of the second random variable given the first $p(y \mid x)$. The ordering of random variables $X$, and $Y$ is arbitrary so the <em>product rule</em> can also be written as</p> \[p(x, y)=p(x \mid y)p(y )\label{eq:3}\] <p>In machine learning and Bayesian statistics, we are interested in making inference of unobserved random variables given that we have observed other random variables.<br/> The <strong>Bayes’ rule or Bayes’ theorem</strong>: If we assume that we have some prior knowledge \(p(x)\) about an unobserved random variable \(x\), and some relationship \(p(y \mid x)\) between \(x\) and second random variable \(y\), which we can observe. If we observe $y$, then we can use Bayes’ theorem to draw some conclusions about \(x\) given the observed values of \(y\).From the direct consequence of the product rule stated above, the <em>bayesian rule states</em>:</p> \[p(x \mid y)p(y)=p(y \mid x)p(x) \Leftrightarrow p(x \mid y)= \frac{p(y \mid x)p(x)}{p(y)} \label{eq:4}\] <p>From above equation, we can deduce following:</p> <ul> <li> <p>\(p(x)\) is the prior which is the probability of an event occurring before new data is collected or in other words, a subjective knowledge of the unobserved variable \(x\) before observing any data. Any prior value can be chosen, however, it is critical to ensure that it has non zero <em>pdf</em> on all plausible \(x\).</p> </li> <li> <p>\(p(y \mid x)\) is the <em>likelihood</em> that describes how \(x\) and \(y\) are related. This likelihood is not a distribution in \(x\) but in \(y\), so we call \(p(x \mid y)\) as “the likelihood (or probability) of $x$ given \(y\)”.</p> </li> <li> <p>\(p(x \mid y)\) is the <em>posterior</em> which is the quantity of interest in Bayesian statistics. Here it is expressed as what we know about \(x\) having observed \(y\). In Bayesian statistics, the “posterior” is a probability that represents our updated beliefs after we have taken into account new evidence. It comes from Bayes’ Rule, which is a way to revise predictions or hypotheses in light of new data.</p> <h4 id="simple-explanation">Simple Explanation</h4> <p>Imagine there’s a 70% chance it will rain today based on the weather forecast (this is our initial belief or “prior”). Then, we look outside and see dark clouds gathering. This new evidence should change our belief about the likelihood of rain. Bayes’ Rule helps us update our beliefs based on new evidence, shifting from our prior belief to our posterior belief.</p> </li> <li> <p>\(p(y)\) is the <em>marginal likelihood/evidence</em> which ensures that the probabilities add up correctly (normalize) to make the posterior probability a true probability measure. It is calculated by considering not just one possible scenario, but all possible scenarios (or parameter values) under a model and seeing how well these scenarios, on average, explain the observed data. Mathematically, it involves summing or integrating the product of the likelihood (how probable the data is if a particular parameter value was true) and the prior (how probable the parameter value is before seeing the data) over all possible parameter values. Mathematically, the evidence is represented as:</p> </li> </ul> \[p(y) =: \int p(y \mid x)p(x)dx = \mathbb{E}_{X}[p(y \mid x)]\] <blockquote> <p>In Bayesian statics, the posterior distribution is the quantity of interest because it represents an updated belief about a parameter after considering both the prior belief and the new evidence provided by the data. If we think of bigger context, then the posterior can be used in decision making under uncertainity, and having full posterior is extremely useful and lead to decisions that are robust to disturbances. Having a full posterior can be very useful for downstream task.</p> </blockquote> <h2 id="expected-value">Expected Value</h2> <p>The concept of expected value is central to machine learning. In probabilistic term, expected value is a mean or average of a random variable incorporating all possible outcomes weighted by their probabilities. Mathematically, the expected value is calculated by summing up all possible values that a random variable can take, each multiplied by its probability of occurrence. The <em>expected value</em> of a function \(g:\mathbb{R}\rightarrow \mathbb{R}\) of a univariate continuous random variable \(X \sim p(x)\) is given by:</p> \[\mathbb{E}_{X}[g(x)] = \int_{x}g(x)p(x)dx\] <p>Correspondingly, the <em>expected value</em> of a function $g$ of a discrete random variable $X \sim p(x)$ is given by:</p> \[\mathbb{E}_{X}[g(x)] = \sum_{x\in\mathcal{X}}g(x)p(x)\] <p>where \(\mathcal{X}\) is a set of possible outcomes of the random variable $X$. Similarly, for a multivariate random variables $X$ defined as a finite vector of univariate random variables $[X_1, X_2, \cdots X_D]^T$, the expected value is defined elemente wise as:</p> \[\begin{align} \mathbb{E}_{X}[g(x)] &amp;= \begin{bmatrix} \mathbb{E}_{X}[g(x_1)] \\ \mathbb{E}_{X}[g(x_2)] \\ \vdots \\ \mathbb{E}_{X}[g(x_D)] \end{bmatrix} \in \mathbb{R} \end{align}\] <p>where the subscript \(\mathbb{E}_{X_d}\) indicates that we are taking the expected value with respect to the \(d^{\text th}\) element of the vector $x$.</p> <h2 id="mean">Mean</h2> <p>The <em>mean</em> of a random variable \(X\) with states \(x \in \mathbb{R}^D\) is an average and is defined as:</p> <p>\(\begin{align} \mathbb{E}_{X}[x] &amp;= \begin{bmatrix} \mathbb{E}_{X_1}[x_1] \\ \mathbb{E}_{X_2}[x_2] \\ \vdots \\ \mathbb{E}_{X_D}[x_D] \end{bmatrix} \in \mathbb{R}^D \end{align}\) $$ where,</p> \[E_{X_d}[x_d]= \begin{cases} \int_{\mathcal{Y}} x_d p(x_d)dx,&amp; \text{if } X \text{ is a continuous random variable}\\ \sum_{x_i\in\mathcal{X}}x_ip(x_d=x_i), &amp; \text{if } X \text{ is a discrete random variable} \end{cases}\] <p>for \(d=1, \cdots, D\), where the subscript \(d\) indicates the corresponding dimension of \(x\). The integral and sum are over the states \(\mathcal{X}\) of the target space of the random variable \(X\).</p> <h2 id="covariance">Covariance</h2> <p>Covariance is calculated as the expected value (average) of the product of the deviations of two random variables from their respective means. Mathematically, the <em>covariance</em> between two univariate random variables \(X\text , Y \in \mathbb{R}\), with mean value \(\mathbb{E}_X[x]\), and \(\mathbb{E}_Y[y]\) respectively can be written as:</p> \[\text{Cov}_{X,Y}[x,y]:= \mathbb{E}_{X,Y}[(x-\mathbb{E}_X[x])(y-\mathbb{E}_Y[y])]\] <p>By using linearity of expectations, the above expressin can be re-written as the expected value of the product minus the product of the expected values as follows:</p> \[\text{Cov}[x,y]=\mathbb{E}[xy]-\mathbb{E}[x]\mathbb{E}[y]\] <blockquote> <p>The covariance of a variable with itself i.e., \(\text{Cov}[x,x]\) is called <em>variance</em> and is denoted by \(\mathbb{V}_X[x]\). And the square root of the variance is called the <em>standard deviation</em>, which is denoted by \(\sigma(x)\).</p> </blockquote> <p><strong>Positive Covariance</strong>: Indicates that as \(X\) increases, \(Y\) also tends to increase. This suggests a positive relationship between the variables.</p> <p><strong>Negative Covariance</strong>: Indicates that as \(X\) increases, \(Y\) tends to decrease. This suggests an inverse relationship between the variables.</p> <p><strong>Zero Covariance</strong>: Implies that there is no linear relationship between \(X\) and \(Y\). The variables are linearly independent, though they could still be related in other non-linear ways.</p>]]></content><author><name></name></author><category term="blog-posts"/><category term="probability"/><category term="statistics"/><summary type="html"><![CDATA[Probability fundamentals for Machine Learning]]></summary></entry><entry><title type="html">Displaying External Posts on Your al-folio Blog</title><link href="https://lakpa-tamang9.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/" rel="alternate" type="text/html" title="Displaying External Posts on Your al-folio Blog"/><published>2022-04-23T23:20:09+00:00</published><updated>2022-04-23T23:20:09+00:00</updated><id>https://lakpa-tamang9.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog</id><content type="html" xml:base="https://lakpa-tamang9.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/"><![CDATA[]]></content><author><name></name></author></entry></feed>